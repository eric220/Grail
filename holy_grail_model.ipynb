{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tfk = tf.keras\n",
    "tf.keras.backend.set_floatx(\"float64\")\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from keras import backend as K\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "from keras import activations, initializers\n",
    "from keras.layers import Layer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, BatchNormalization, Activation, LeakyReLU\n",
    "from tensorflow_probability.python.layers import DistributionLambda\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = 1\n",
    "def neg_log_likelihood_with_dist(y_true, y_pred, sigma = noise):\n",
    "    return -tf.reduce_mean(y_pred.log_prob(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixture_prior_params(sigma_1, sigma_2, pi, return_sigma=False):\n",
    "    params = K.variable([sigma_1, sigma_2, pi], name='mixture_prior_params')\n",
    "    sigma = np.sqrt(pi * sigma_1 ** 2 + (1 - pi) * sigma_2 ** 2)\n",
    "    return params, sigma\n",
    "  \n",
    "def log_mixture_prior_prob(w):\n",
    "    comp_1_dist = tf.distributions.Normal(0.0, prior_params[0])\n",
    "    comp_2_dist = tf.distributions.Normal(0.0, prior_params[1])\n",
    "    comp_1_weight = prior_params[2]\n",
    "    return K.log(comp_1_weight * comp_1_dist.prob(w) + (1 - comp_1_weight) * comp_2_dist.prob(w))\n",
    "  \n",
    "# Mixture prior parameters shared across DenseVariational layer instances\n",
    "prior_params, prior_sigma = mixture_prior_params(sigma_1=1.0, sigma_2=0.1, pi=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the surrogate posterior over `keras.layers.Dense` `kernel` and `bias`.\n",
    "def  posterior_mean_field(kernel_size, bias_size=0, dtype=None):\n",
    "    n = kernel_size + bias_size\n",
    "    c = np.log(np.expm1(1.))\n",
    "    return tf.keras.Sequential(\n",
    "        [tfp.layers.VariableLayer(2 * n, dtype=dtype),\n",
    "        tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n",
    "            tfd.Normal(loc=t[..., :n], scale=1e-5 + tf.nn.softplus(c + t[..., n:])),\n",
    "            reinterpreted_batch_ndims=1)),\n",
    "])\n",
    "\n",
    "# Specify the prior over `keras.layers.Dense` `kernel` and `bias`.\n",
    "def prior_trainable(kernel_size, bias_size=0, dtype=None):\n",
    "    n = kernel_size + bias_size\n",
    "    return tf.keras.Sequential([\n",
    "        tfp.layers.VariableLayer(n, dtype=dtype),\n",
    "        tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n",
    "            tfd.Normal(loc=t, scale=1),\n",
    "            reinterpreted_batch_ndims=1)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('scaled_dummied_task.csv')\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "permuts = pd.read_csv('../HolyGrail/permutations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df.drop('Target', axis = 1)\n",
    "y_train = df['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100000\n",
    "lr = 5e-3\n",
    "batch_size = x_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implement_wape(ys, preds):\n",
    "    print(np.array(ys) - np.array(preds))\n",
    "    numer8 = np.sum(np.abs(np.array(ys) - np.array(preds)))\n",
    "    print(numer8)\n",
    "    w = (numer8)/np.sum(np.abs(ys))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Params testing: ', {'units1': 32, 'units2': 32, 'optimizer': 'adadelta', 'choice': {'layers': 'two'}})\n",
      "  0%|          | 0/5 [00:00<?, ?trial/s, best loss=?]WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <type 'NoneType'>\n",
      "Train on 600 samples                                 \n",
      "Epoch 1/5                                            \n",
      "600/600 [==============================]             \n",
      " - 1s 2ms/sample - loss: 807.1666 - mse: 763.0327    \n",
      "\n",
      "Epoch 2/5                                            \n",
      "600/600 [==============================]             \n",
      " - 0s 18us/sample - loss: 639.5879 - mse: 727.7476   \n",
      "\n",
      "Epoch 3/5                                            \n",
      "600/600 [==============================]             \n",
      " - 0s 19us/sample - loss: 1227.6436 - mse: 1113.7015 \n",
      "\n",
      "Epoch 4/5                                            \n",
      "600/600 [==============================]             \n",
      " - 0s 19us/sample - loss: 492.1997 - mse: 473.0834   \n",
      "\n",
      "Epoch 5/5                                            \n",
      "600/600 [==============================]             \n",
      " - 0s 18us/sample - loss: 3463.0496 - mse: 3485.2888 \n",
      "\n",
      "[-4.16466914  0.4364335 ]                            \n",
      "4.601102643772406                                    \n",
      "WAPE: 0.990106703395                                 \n",
      "('Params testing: ', {'units1': 32, 'units2': 16, 'optimizer': 'rmsprop', 'choice': {'layers': 'two'}})\n",
      " 20%|██        | 1/5 [00:05<00:19,  5.00s/trial, best loss: 0.990106703395]WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <type 'NoneType'>\n",
      "Train on 600 samples                                                       \n",
      "Epoch 1/5                                                                  \n",
      "600/600 [==============================]                                   \n",
      " - 1s 2ms/sample - loss: 665.2353 - mse: 642.7833                          \n",
      "\n",
      "Epoch 2/5                                                                  \n",
      "600/600 [==============================]                                   \n",
      " - 0s 19us/sample - loss: 1862.9779 - mse: 1821.7913                       \n",
      "\n",
      "Epoch 3/5                                                                  \n",
      "600/600 [==============================]                                   \n",
      " - 0s 18us/sample - loss: 558.0576 - mse: 535.0931                         \n",
      "\n",
      "Epoch 4/5                                                                  \n",
      "600/600 [==============================]                                   \n",
      " - 0s 18us/sample - loss: 3377.7073 - mse: 3248.3586                       \n",
      "\n",
      "Epoch 5/5                                                                  \n",
      "600/600 [==============================]                                   \n",
      " - 0s 20us/sample - loss: 330.3896 - mse: 314.3341                         \n",
      "\n",
      "[23.57593335  8.43365068]                                                  \n",
      "32.0095840289371                                                           \n",
      "WAPE: 0.997271998758                                                       \n",
      "('Params testing: ', {'units1': 32, 'units2': 64, 'optimizer': 'rmsprop', 'choice': {'layers': 'three', 'units3': 32}})\n",
      " 40%|████      | 2/5 [00:09<00:14,  4.85s/trial, best loss: 0.990106703395]WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <type 'NoneType'>\n",
      "Train on 600 samples                                                       \n",
      "Epoch 1/5                                                                  \n",
      "600/600 [==============================]                                   \n",
      " - 1s 2ms/sample - loss: 122163.8672 - mse: 85493.2500                     \n",
      "\n",
      "Epoch 2/5                                                                  \n",
      "600/600 [==============================]                                   \n",
      " - 0s 24us/sample - loss: 169289.6250 - mse: 137502.6719                   \n",
      "\n",
      "Epoch 3/5                                                                  \n",
      "600/600 [==============================]                                   \n",
      " - 0s 22us/sample - loss: 23877.8535 - mse: 20879.5938                     \n",
      "\n",
      "Epoch 4/5                                                                  \n",
      "600/600 [==============================]                                   \n",
      " - 0s 22us/sample - loss: 53121.5703 - mse: 49563.8594                     \n",
      "\n",
      "Epoch 5/5                                                                  \n",
      "600/600 [==============================]                                   \n",
      " - 0s 22us/sample - loss: 58624.6797 - mse: 62624.1055                     \n",
      "\n",
      "[-139.58091174   71.73941583]                                              \n",
      "211.32032757358198                                                         \n",
      "WAPE: 0.999782487009                                                       \n",
      "('Params testing: ', {'units1': 64, 'units2': 32, 'optimizer': 'rmsprop', 'choice': {'layers': 'two'}})\n",
      " 60%|██████    | 3/5 [00:15<00:10,  5.24s/trial, best loss: 0.990106703395]WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <type 'NoneType'>\n",
      "Train on 600 samples                                                       \n",
      "Epoch 1/5                                                                  \n",
      "600/600 [==============================]                                   \n",
      " - 1s 2ms/sample - loss: 2421.3621 - mse: 2197.8337                        \n",
      "\n",
      "Epoch 2/5                                                                  \n",
      "600/600 [==============================]                                   \n",
      " - 0s 20us/sample - loss: 1609.8817 - mse: 1495.9042                       \n",
      "\n",
      "Epoch 3/5                                                                  \n",
      "600/600 [==============================]                                   \n",
      " - 0s 19us/sample - loss: 5152.7085 - mse: 4613.1284                       \n",
      "\n",
      "Epoch 4/5                                                                  \n",
      "600/600 [==============================]                                   \n",
      " - 0s 19us/sample - loss: 15547.7197 - mse: 16510.6387                     \n",
      "\n",
      "Epoch 5/5                                                                  \n",
      "600/600 [==============================]                                   \n",
      " - 0s 23us/sample - loss: 1609.5155 - mse: 1496.0771                       \n",
      "\n",
      "[ 20.04232396 -17.1036666 ]                                                \n",
      "37.14599055691119                                                          \n",
      "WAPE: 1.00123926703                                                        \n",
      "('Params testing: ', {'units1': 32, 'units2': 16, 'optimizer': 'adadelta', 'choice': {'layers': 'two'}})\n",
      " 80%|████████  | 4/5 [00:20<00:05,  5.07s/trial, best loss: 0.990106703395]WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <type 'NoneType'>\n",
      "Train on 600 samples                                                       \n",
      "Epoch 1/5                                                                  \n",
      "600/600 [==============================]                                   \n",
      " - 1s 2ms/sample - loss: 658.6440 - mse: 676.9763                          \n",
      "\n",
      "Epoch 2/5                                                                  \n",
      "600/600 [==============================]                                   \n",
      " - 0s 17us/sample - loss: 1364.8040 - mse: 1311.4312                       \n",
      "\n",
      "Epoch 3/5                                                                  \n",
      "600/600 [==============================]                                   \n",
      " - 0s 18us/sample - loss: 183.3844 - mse: 181.5867                         \n",
      "\n",
      "Epoch 4/5                                                                  \n",
      "600/600 [==============================]                                   \n",
      " - 0s 18us/sample - loss: 5600.5283 - mse: 5547.0103                       \n",
      "\n",
      "Epoch 5/5                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================]                                   \n",
      " - 0s 18us/sample - loss: 326.9134 - mse: 323.0713                         \n",
      "\n",
      "[8.64262379 5.09320325]                                                    \n",
      "13.735827048041102                                                         \n",
      "WAPE: 0.993665868618                                                       \n",
      "100%|██████████| 5/5 [00:26<00:00,  5.20s/trial, best loss: 0.990106703395]\n",
      "('best: ', {'units1': 1, 'units2': 1, 'optimizer': 0, 'num_layers': 0})\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import sys\n",
    "train_size = len(x_train)\n",
    "X = []\n",
    "y = []\n",
    "X_val = []\n",
    "y_val = []\n",
    "\n",
    "space = {'choice': hp.choice('num_layers',\n",
    "                             [{'layers':'two', },\n",
    "                              {'layers':'three', \n",
    "                               'units3': hp.choice('units3', [32, 64])\n",
    "                              }\n",
    "                             ]),\n",
    "         'units1': hp.choice('units1', [16, 32, 64]),\n",
    "         'units2': hp.choice('units2', [16, 32, 64]),\n",
    "         'optimizer': hp.choice('optimizer',['adadelta','adam','rmsprop']),\n",
    "        }\n",
    "\n",
    "def f_nn(params):   \n",
    "\n",
    "    print ('Params testing: ', params)\n",
    "    model_in = Input(shape=(x_train.shape[1],))\n",
    "    x = tfp.layers.DenseVariational(int('{}'.format(params['units1'])), posterior_mean_field, prior_trainable, kl_weight=1/train_size)(model_in)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "    x = tfp.layers.DenseVariational(int('{}'.format(params['units2'])), posterior_mean_field, prior_trainable, kl_weight=1/train_size)(x)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "    \n",
    "    if params['choice']['layers']== 'three':\n",
    "        x = tfp.layers.DenseVariational(int('{}'.format(params['choice']['units3'])), posterior_mean_field, prior_trainable, kl_weight=1/train_size)(x)\n",
    "        x = LeakyReLU(0.1)(x) \n",
    "    \n",
    "    model_out_loc = tfp.layers.DenseVariational(1, posterior_mean_field, prior_trainable, kl_weight=1/train_size)(x)\n",
    "    model_out_scale = tfp.layers.DenseVariational(1, posterior_mean_field, prior_trainable, kl_weight=1/train_size)(x)\n",
    "    model_out = DistributionLambda(lambda t: tfd.Normal(loc=t[0],\n",
    "                                                        scale=1e-7 + tf.math.softplus(1e-3 * t[1])))([model_out_loc,\n",
    "                                                                                                      model_out_scale])\n",
    "    model = Model(model_in, model_out)\n",
    "    model.compile(loss=neg_log_likelihood_with_dist, optimizer=params['optimizer'], metrics=['mse'])\n",
    "\n",
    "    model.fit(x_train, y_train, epochs = n_epochs, batch_size = batch_size, verbose = 1)\n",
    "\n",
    "    y_pred_list = []\n",
    "    for i in range(len(x_train)):\n",
    "        tensor = tf.constant([x_train.iloc[i]], dtype='float32')\n",
    "        pred_list = []\n",
    "        for j in range(500):\n",
    "            y_pred = model.predict(tensor)\n",
    "            pred_list.append(y_pred)\n",
    "        y_pred_list.append(pred_list)\n",
    "\n",
    "    y_means = np.mean(y_pred_list, axis = 1)\n",
    "    y_means = y_means.reshape(len(x_train),)\n",
    "    wape = implement_wape(y_means, y_train)\n",
    "    print('WAPE: {}' .format(wape))\n",
    "    return wape\n",
    "    \n",
    "trials = Trials()\n",
    "best = fmin(f_nn, space, algo=tpe.suggest, max_evals=5, trials=trials)\n",
    "print('best: ', best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "for i in range(len(permuts)):\n",
    "    tensor = tf.constant([permuts.iloc[i]], dtype='float32')\n",
    "    pred_list = []\n",
    "    for j in range(5):\n",
    "        y_pred = model.predict(tensor)\n",
    "        pred_list.append(y_pred)\n",
    "    y_pred_list.append(pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_means = np.mean(y_pred_list, axis = 1)\n",
    "y_sigmas = np.std(y_pred_list, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sigmas = y_sigmas.reshape(len(permuts),)\n",
    "y_means = y_means.reshape(len(permuts),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = range(len(permuts))\n",
    "plt.figure(figsize = (15, 7))\n",
    "plt.plot(idx, y_means, 'r-', label='Predictive mean');\n",
    "plt.fill_between(idx, y_means + 2 * y_sigmas,\n",
    "                 y_means - 2 * y_sigmas, alpha=0.5)\n",
    "plt.legend()\n",
    "plt.title('Predictions');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
